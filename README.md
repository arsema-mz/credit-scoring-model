# Credit Risk Probability Model using Alternative Data

## ğŸ“Œ Project Overview
This project develops and deploys a **Credit Risk Probability Model** leveraging **alternative data** sources (e-commerce & bank transactions). The goal is to predict the probability of default (PD) for customers and support better credit risk assessment in line with **Basel II standards**.

The pipeline covers the **entire ML lifecycle**:
- Data preprocessing & feature engineering
- Model training & evaluation
- Explainability with SHAP
- Deployment via **FastAPI** & **Streamlit**
- CI/CD with GitHub Actions & Docker


## ğŸš€ Project Structure

credit-scoring-model/
â”‚â”€â”€ data/                   # Raw & processed datasets
â”‚â”€â”€ models/                 # Trained ML models
â”‚â”€â”€ notebooks/              # EDA & experimentation
â”‚â”€â”€ src/                    # Core ML pipeline
â”‚   â”œâ”€â”€ data\_processing.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ predict.py
â”‚â”€â”€ app/                    # FastAPI app for deployment
â”‚   â”œâ”€â”€ main.py
â”‚â”€â”€ streamlit\_app.py         # Streamlit dashboard
â”‚â”€â”€ tests/                   # Unit tests
â”‚â”€â”€ assets/                  # Screenshots & diagrams
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ README.md



## ğŸ› ï¸ Tech Stack
- **Python 3.12**
- **scikit-learn, XGBoost** â€“ Modeling
- **pandas, numpy** â€“ Data preprocessing
- **SHAP** â€“ Model explainability
- **FastAPI** â€“ API deployment
- **Streamlit** â€“ Interactive dashboard
- **Docker & GitHub Actions** â€“ Containerization + CI/CD


## ğŸ“Š Key Features
1. **Data Pipeline** â€“ Automated preprocessing (feature scaling, encoding, imputation).
2. **Model Training** â€“ Logistic Regression & Random Forest (with best hyperparameters).
3. **Explainability** â€“ SHAP summary plots for feature importance.
4. **Deployment**  
   - **FastAPI** â€“ REST API for predictions.  
   - **Streamlit App** â€“ Interactive UI for users to upload data & view predictions.  
5. **Automation** â€“ CI/CD with GitHub Actions & Docker.


## âš¡ How to Run

### 1ï¸âƒ£ Setup Environment
```bash
git clone https://github.com/arsema-mz/credit-scoring-model.git
cd credit-scoring-model
pip install -r requirements.txt
````

### 2ï¸âƒ£ Train Models

```bash
python src/train.py
```

### 3ï¸âƒ£ Run Predictions

```bash
# Run predictions on new data
python src/predict.py --model both --input data/new/new_data.csv --output data/predictions/predicted.csv
```

### 4ï¸âƒ£ Start FastAPI Service

```bash
uvicorn app.main:app --reload
```

API available at ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

### 5ï¸âƒ£ Launch Streamlit Dashboard

```bash
streamlit run streamlit_app.py
```


## ğŸ–¼ï¸ Screenshots

### Streamlit Dashboard

![Streamlit App](assets/Screenshot%202025-08-20%20101118.png)



## ğŸ“ˆ Results

* Logistic Regression & Random Forest trained & evaluated
* SHAP explainability identified **transaction frequency & geolocation mismatch** as key fraud/risk drivers
* Interactive dashboard for real-time credit risk scoring


## ğŸ³ Docker Deployment

```bash
docker build -t credit-risk-model .
docker run -p 8000:8000 credit-risk-model
```
